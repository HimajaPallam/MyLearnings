Kafka provided event streaming platform
Topics - each topic have many partitions
	   - can have many producers and consumer
	   - Its like file system/directory) and partitions are files
	   - Each partition is distributed among brokers -> 1 partition - 1 brokers
	   
Producer - exactly once - delivery:
-----------------------------------------
 The Kafka producer also supports an idempotent delivery option which guarantees that resending will not result in duplicate entries in the log. To achieve this, the broker assigns each producer an ID and deduplicates messages using a sequence number that is sent by the producer along with every message. Also beginning with 0.11.0.0, the producer supports the ability to send messages to multiple topic partitions using transaction-like semantics: i.e. either all messages are successfully written or none of them are. The main use case for this is exactly-once processing between Kafka topics 
 

Consumer - Read - Exactly once:
-----------------------------------------
The consumer controls its position in this log. If the consumer never crashed it could just store this position in memory, but if the consumer fails and we want this topic partition to be taken over by another process the new process will need to choose an appropriate position from which to start processing. 

It can read the messages, then save its position in the log, and finally process the messages. In this case there is a possibility that the consumer process crashes after saving its position but before saving the output of its message processing. In this case the process that took over processing would start at the saved position even though a few messages prior to that position had not been processed. This corresponds to "at-most-once" semantics 

It can read the messages, process the messages, and finally save its position. In this case there is a possibility that the consumer process crashes after processing messages but before saving its position. In this case when the new process takes over the first few messages it receives will already have been processed. This corresponds to the "at-least-once" semantics in the case of consumer failure.

So what about exactly once semantics (i.e. the thing you actually want)? When consuming from a Kafka topic and producing to another topic (as in a Kafka Streams application), we can leverage the new transactional producer capabilities in 0.11.0.0 that were mentioned above. The consumer's position is stored as a message in a topic, so we can write the offset to Kafka in the same transaction as the output topics receiving the processed data. If the transaction is aborted, the consumer's position will revert to its old value and the produced data on the output topics will not be visible to other consumers, depending on their "isolation level." In the default "read_uncommitted" isolation level, all messages are visible to consumers even if they were part of an aborted transaction, but in "read_committed," the consumer will only return messages from transactions which were committed (and any messages which were not part of a transaction).

When writing to an external system, the limitation is in the need to coordinate the consumer's position with what is actually stored as output. The classic way of achieving this would be to introduce a two-phase commit between the storage of the consumer position and the storage of the consumers output. But this can be handled more simply and generally by letting the consumer store its offset in the same place as its output. This is better because many of the output systems a consumer might want to write to will not support a two-phase commit. As an example of this, consider a Kafka Connect connector which populates data in HDFS along with the offsets of the data it reads so that it is guaranteed that either data and offsets are both updated or neither is.

So effectively Kafka supports exactly-once delivery in Kafka Streams, and the transactional producer/consumer can be used generally to provide exactly-once delivery when transferring and processing data between Kafka topics. Exactly-once delivery for other destination systems generally requires cooperation with such systems, but Kafka provides the offset which makes implementing this feasible (see also Kafka Connect). Otherwise, Kafka guarantees at-least-once delivery by default, and allows the user to implement at-most-once delivery by disabling retries on the producer and committing offsets in the consumer prior to processing a batch of messages.


So what about exactly once semantics (i.e. the thing you actually want)? When consuming from a Kafka topic and producing to another topic (as in a Kafka Streams application), we can leverage the new transactional producer capabilities in 0.11.0.0 that were mentioned above. The consumer's position is stored as a message in a topic, so we can write the offset to Kafka in the same transaction as the output topics receiving the processed data. If the transaction is aborted, the consumer's position will revert to its old value and the produced data on the output topics will not be visible to other consumers, depending on their "isolation level." In the default "read_uncommitted" isolation level, all messages are visible to consumers even if they were part of an aborted transaction, but in "read_committed," the consumer will only return messages from transactions which were committed (and any messages which were not part of a transaction).

When writing to an external system, the limitation is in the need to coordinate the consumer's position with what is actually stored as output. The classic way of achieving this would be to introduce a two-phase commit between the storage of the consumer position and the storage of the consumers output. But this can be handled more simply and generally by letting the consumer store its offset in the same place as its output. This is better because many of the output systems a consumer might want to write to will not support a two-phase commit. As an example of this, consider a Kafka Connect connector which populates data in HDFS along with the offsets of the data it reads so that it is guaranteed that either data and offsets are both updated or neither is. We follow similar patterns for many other data systems which require these stronger semantics and for which the messages do not have a primary key to allow for deduplication.

So effectively Kafka supports exactly-once delivery in Kafka Streams, and the transactional producer/consumer can be used generally to provide exactly-once delivery when transferring and processing data between Kafka topics. Exactly-once delivery for other destination systems generally requires cooperation with such systems, but Kafka provides the offset which makes implementing this feasible (see also Kafka Connect). Otherwise, Kafka guarantees at-least-once delivery by default, and allows the user to implement at-most-once delivery by disabling retries on the producer and committing offsets in the consumer prior to processing a batch of messages.


Kafka replicates the log for each topic's partitions across a configurable number of servers (you can set this replication factor on a topic-by-topic basis).
Under non-failure conditions, each partition in Kafka has a single leader and zero or more followers. The total number of replicas including the leader constitute the replication factor. All writes go to the leader of the partition, and reads can go to the leader or the followers of the partition. Typically, there are many more partitions than brokers and the leaders are evenly distributed among brokers. The logs on the followers are identical to the leader's logâ€”all have the same offsets and messages in the same order 





Kafka Producer:
-----------------------------

H2 Database connection to Kafka Producer:

Step1:

1. Select the tables from which we need live feeds. Decide on the condition that should trigger the capture of live feeds. For example, you might want to capture changes whenever a specific table is updated or a certain column meets certain criteria.

-- query to trigger the table on updating any of the data
	CREATE TRIGGER your_trigger_name
	AFTER UPDATE ON your_table
	FOR EACH ROW
	CALL "your_java_class_name.your_trigger_method";
	
OR

Configure the H2 database trigger to make an HTTP request:
Modify your H2 database trigger to make an HTTP request to the API endpoint exposed by your Tomcat project. H2 database triggers can execute SQL statements, so you can use the CALL HTTP('url', 'method', 'headers', 'body') function to make the HTTP request. Here's an example:

	CREATE TRIGGER your_trigger_name
	AFTER UPDATE ON your_table
	FOR EACH ROW
	CALL HTTP('http://localhost:8080/h2trigger', 'POST', 'Content-Type:application/x-www-form-urlencoded', 'key=' || NEW.key_column || '&value=' || NEW.value_column);

OR

-- query to trigger the table on updating two of the columns
CREATE TRIGGER your_trigger_name
AFTER UPDATE ON your_table
FOR EACH ROW
WHEN (NEW.column1 <> OLD.column1 OR NEW.column2 <> OLD.column2)
CALL HTTP('http://localhost:8080/h2trigger', 'POST', 'Content-Type:application/x-www-form-urlencoded', 'key=' || NEW.key_column || '&value=' || NEW.value_column);




2. Implement the Trigger Logic in Java:
Create a Java class that contains the logic for capturing the live feeds.
Implement the specified trigger method that will be called by the database trigger. This method should receive the necessary information about the updated data.
Inside the trigger method, you can create a Kafka producer instance to send the updated data to a Kafka topic.

@RestController
public class H2TriggerController {

    @PostMapping("/h2trigger")
    public ResponseEntity<String> handleH2Trigger(@RequestParam String key, @RequestParam String value) {
        // Send data to Kafka
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

        try (Producer<String, String> producer = new KafkaProducer<>(props)) {
            ProducerRecord<String, String> record = new ProducerRecord<>("your_topic", key, value);
            producer.send(record);
        }

        return ResponseEntity.ok("Data received and processed successfully");
    }
}

3. To enable the H2 database to invoke the Java method when the trigger condition is met, you need to register the Java class in the database.
Use the H2 CREATE ALIAS statement to create an alias that points to your Java class and the trigger method. Here's an example:

	CREATE ALIAS your_trigger_method FOR "your_java_class_name.your_trigger_method";
	


Kafka Consumer:
-----------------------------

1. Configure Kafka Consumer Properties:
In your Spring Boot application, define the necessary Kafka consumer properties in the application configuration file (application.properties or application.yml). 

spring.kafka.bootstrap-servers=localhost:9092
spring.kafka.consumer.group-id=your_consumer_group_id
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer


2. Create a Kafka Consumer Listener:
Define a Kafka consumer listener class that will handle the incoming Kafka messages. This listener will be responsible for processing the messages received from the Kafka topic. You can annotate this class with @KafkaListener and specify the topic(s) to subscribe to. For example:
	import org.springframework.kafka.annotation.KafkaListener;
	import org.springframework.stereotype.Component;

	@Component
	public class KafkaMessageListener {

		@KafkaListener(topics = "your_topic")
		public void listen(ConsumerRecord<String, String> record) {
			String key = record.key();
			String value = record.value();
			// Process the key and value as needed
			System.out.println("Received message: Key=" + key + ", Value=" + value);
		}
	}



